# Build a ML Workflow For Scones Unlimited On Amazon SageMaker

This project involves building a complete machine learning workflow for "Scones Unlimited" using Amazon SageMaker. The workflow includes setting up a SageMaker Studio environment, preparing data, training and deploying a model, creating API endpoints, authoring Lambda functions, composing them in a Step Function, and monitoring the deployed model. Below is a detailed overview of the steps taken and the criteria met in each stage of the project.

## 1. Setup SageMaker Studio

### SageMaker Studio Workspace
- **Setup:** A SageMaker Studio workspace was set up to run the project. A compatible kernel was selected to execute the provided notebooks and scripts.

## 2. Data Preparation

### Extract, Transform, Load (ETL)
- **Data Loading:** The ETL process was completed using the starter code provided. This step involved extracting, transforming, and loading the data into a format suitable for training a machine learning model in SageMaker.

## 3. Train and Deploy a Machine Learning Model

### Model Training
- **Training:** The image classification model was trained successfully using the provided training data. The training process was completed up to the "Getting ready to deploy" section, indicating that the model is ready for deployment.

### Model Deployment
- **API Endpoint:** The trained model was deployed on SageMaker, and an API endpoint was created. The endpoint's unique name was printed in the notebook for reference.
- **Model Inference:** A sample image was used to make predictions via the deployed model, verifying that the endpoint is functioning correctly.

## 4. Build a Full Machine Learning Workflow

### Lambda Functions
- **Authoring Lambda Functions:** Three Lambda functions were authored to perform specific tasks within the workflow:
  1. **First Lambda:** This function is responsible for returning an object to the step function as `image_data` in an event.
  2. **Second Lambda:** This function handles image classification using the deployed model.
  3. **Third Lambda:** This function filters out low-confidence inferences.
- **Code Storage:** The code for each Lambda function was saved in separate Python scripts for easy reference and reuse.

### Step Function
- **Compose Step Function:** The Lambda functions were composed into a Step Function. The Step Function was defined in JSON format and exported for documentation and review.
- **Validation:** A screenshot of the working Step Function was taken to confirm its correct operation.

## 5. Monitor the Model for Errors

### Model Monitoring Data
- **Data Extraction:** Monitoring data generated by the Model Monitor was successfully loaded from S3 into the notebook. This data is crucial for understanding the model's performance and detecting any potential issues.

### Data Visualization
- **Visualization:** Custom visualizations were created to represent the Model Monitor data outputs. These visualizations help in analyzing the model's behavior and ensuring it operates as expected.
